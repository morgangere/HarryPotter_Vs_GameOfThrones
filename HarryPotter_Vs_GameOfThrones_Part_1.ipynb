{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490ab3a0",
   "metadata": {},
   "source": [
    "    As an introduction to natural language processing two texts will be obtained, preprocessed, tokenized, normalized and the most common tokens representing the texts will be examined.  These texts will then be subjected to the same process using bigrams and the top normalized tokenized bigrams will be examined.  The steps and decisions will be explained along the way.  \n",
    "\n",
    "    The two texts are a Harry Potter and the Philosophers Stone by J. K. Rowling and Game of Thrones (A song of Ice and Fire, Book 1).  Both books are the first book in a series, and both are in the fantasy genre.  The differences between them are that one is for adult audiences, and one is for children.  The overall focus of the fantasy is also different, along with the time period and the use of fiction.  It is the goal to be able to see major differences in vocabulary representing the intended age range the books were designed for.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b4c28",
   "metadata": {},
   "source": [
    "    (1b) A text file containing the first Harry Potter book was obtained from Kaggle (https://www.kaggle.com/datasets/balabaskar/harry-potter-books-corpora-part-1-7).  It was read into python and the text was inspected to determine preprocessing steps.  An example section of the text is provided to show some of the preprocessing required.  There are many things that need to be preprocessed before the tokenization and normalization can be performed.  There are lines that are skipped (\\n), page numbers, and some strange characters that appeared later in the text that required removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9f95bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ \\n\\n\\n\\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, \\nwere proud to say that they were perfectly normal, \\nthank you very much. They were the last people you’d \\nexpect to be involved in anything strange or \\nmysterious, because they just didn’t hold with such \\nnonsense. \\n\\nMr. Dursley was the director of a firm called \\nGrunnings, which made drills. He was a big, beefy \\nman with hardly any neck, although he did have a \\nvery large mustache. Mrs. Dursley was thin and \\nblonde and had nearly twice the usual amount of \\nneck, which came in very useful as she spent so \\nmuch of her time craning over garden fences, spying \\non the neighbors. The Dursley s had a small son \\ncalled Dudley and in their opinion there was no finer \\nboy anywhere. \\n\\nThe Dursleys had everything they wanted, but they \\nalso had a secret, and their greatest fear was that \\nsomebody would discover it. They didn’t think they \\ncould bear it if anyone found out about the Potters. \\nMrs. Potter was Mrs. Dursley’s sister, but they hadn’t \\n\\nPage | 2 Harry Potter and the Philosophers Stone - J.K. Rowling \\n\\n\\n\\nmet for several years; in fact, Mrs. Dursley pretended \\nshe didn’t have a sister, because her sister '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the path\n",
    "hp_path = r'C:\\Users\\Morga\\programsMG\\NaturalLanguageProcessing\\book1.txt'\n",
    "\n",
    "# open the file\n",
    "f = open(hp_path, 'r',encoding=\"utf8\")\n",
    "\n",
    "#reading the text saving it to a variable\n",
    "hptext = f.read()\n",
    "\n",
    "#displaying the section of the text\n",
    "hptext[:1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493eb94",
   "metadata": {},
   "source": [
    "    (2)Regular expressions patterns were created and saved using re.compile.  These patterns were placed inside of re.sub and the text was removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f2866fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Removing the \\n\n",
    "pattern = re.compile('\\n')\n",
    "hptext2=re.sub(pattern,'',hptext)\n",
    "\n",
    "#removing Page | ### Harry Potter and the Philosophers Stone - J.K. Rowling\n",
    "pattern2 = re.compile('Page \\| [0-9][0-9]*[0-9]* Harry Potter and the Philosophers Stone - *J.K. Rowling')\n",
    "hptext2=re.sub(pattern2,'',hptext2)\n",
    "\n",
    "#removing this wierd k k k  and symbol k k k\n",
    "pattern3 = re.compile('•k k k|k k k')\n",
    "hptext2=re.sub(pattern3,'',hptext2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e05580",
   "metadata": {},
   "source": [
    "    There remain a few items at the beginning and end of the text that require removal.  These were removed directly after tokenization.  Using nltk word tokenizer the text was broken in word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6053f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "import nltk\n",
    "hptokens = nltk.word_tokenize(hptext2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ecc05",
   "metadata": {},
   "source": [
    "    The beginning and end of the token list was examined to see the tokens that were created that are not part of the actual book but are from the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c2193b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101215\n",
      "['/', 'THE', 'BOY', 'WHO', 'LIVED', 'Mr.', 'and', 'Mrs.', 'Dursley', ',']\n",
      "['’', 'm', 'going', 'to', 'have', 'a', 'lot', 'of', 'fun', 'with', 'Dudley', 'this', 'summer', '...', '”']\n"
     ]
    }
   ],
   "source": [
    "#showing the beginging and ending\n",
    "print(len(hptokens))\n",
    "print(hptokens[:10])\n",
    "print(hptokens[101200:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a8444",
   "metadata": {},
   "source": [
    "    These tokens are manually deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d4ee697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['’', 'm', 'going', 'to', 'have', 'a', 'lot', 'of', 'fun', 'with', 'Dudley', 'this', 'summer', '...']\n"
     ]
    }
   ],
   "source": [
    "#the last token is not from the book needs to be removed\n",
    "del hptokens[101214]\n",
    "print(hptokens[101200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9107cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE', 'BOY', 'WHO', 'LIVED', 'Mr.', 'and', 'Mrs.', 'Dursley', ',', 'of']\n"
     ]
    }
   ],
   "source": [
    "#The first token is not from the book needs to be removed\n",
    "del hptokens[0]\n",
    "print(hptokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28e53e",
   "metadata": {},
   "source": [
    "    (1b)A text file containing the first Game of Thrones book was obtained from Kaggle (https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books).  Again this data was read into python and the text was inspected to determine preprocessing steps.  An example section of the text is provided to show some of the preprocessing required.  There are many things that need to be preprocessed before the tokenization and normalization can be performed.  There are lines that are skipped (\\n) and page numbers.  It was also found that there were numbers replacing letters in the text itself.  There are o represented by 0.  I and L represented by 1s and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c321162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Game Of Thrones \\nBook One of A Song of Ice and Fire \\nBy George R. R. Martin \\nPROLOGUE \\n\"We should start back,\" Gared urged as the woods began to grow dark around them. \"The wildlings are \\ndead.\" \\n\"Do the dead frighten you?\" Ser Waymar Royce asked with just the hint of a smile. \\nGared did not rise to the bait. He was an old man, past fifty, and he had seen the lordlings come and go. \\n\"Dead is dead,\" he said. \"We have no business with the dead.\" \\n\"Are they dead?\" Royce asked softly. \"What proof have we?\" \\n\"Will saw them,\" Gared said. \"If he says they are dead, that\\'s proof enough for me.\" \\nWill had known they would drag him into the quarrel sooner or later. He wished it had been later rather \\nthan sooner. \"My mother told me that dead men sing no songs,\" he put in. \\n\"My wet nurse said the same thing, Will,\" Royce replied. \"Never believe anything you hear at a woman\\'s \\ntit. There are things to be learned even from the dead.\" His voice echoed, too loud in the twilit forest. \\nPage 1\\n\\n\"We h'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the path\n",
    "got_path = r'C:\\Users\\Morga\\programsMG\\NaturalLanguageProcessing\\001ssb.txt'\n",
    "\n",
    "# open the file\n",
    "f = open(got_path, 'r',encoding=\"utf8\")\n",
    "\n",
    "#reading the text and saving it to a variable\n",
    "gottext = f.read()\n",
    "\n",
    "#displaying the section of the text\n",
    "gottext[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1414f4",
   "metadata": {},
   "source": [
    "    (2)Regular expressions patterns were created and saved using re.compile.  These patterns were placed inside of re.sub and the text was removed. There was a large amount of preprocessing required for this book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f875d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the \\n\n",
    "pattern = re.compile('\\n')\n",
    "gottext2=re.sub(pattern,'',gottext)\n",
    "\n",
    "\n",
    "## Removing page numbers and aurhtor from the pages along with some random numbers typed it the text\n",
    "pattern2 = re.compile('[Pp]age [0-9][0-9]*[0-9]*')\n",
    "gottext2=re.sub(pattern2,'',gottext2)\n",
    "\n",
    "pattern3 = re.compile('1fm')\n",
    "gottext2=re.sub(pattern3,\"I'm\",gottext2)\n",
    "\n",
    "pattern4 = re.compile('m1ord')\n",
    "gottext2=re.sub(pattern4,\"mlord\",gottext2)\n",
    "\n",
    "pattern5 = re.compile('11 \\*')\n",
    "gottext2=re.sub(pattern5,\"I'll\",gottext2)\n",
    "\n",
    "pattern6 = re.compile('Aerys 11 Targaryen')\n",
    "gottext2=re.sub(pattern6,\"Aerys II Targaryen\",gottext2)\n",
    "\n",
    "pattern7 = re.compile('Aegon 111')\n",
    "gottext2=re.sub(pattern7,\"Aegon III\",gottext2)\n",
    "\n",
    "pattern8 = re.compile('11IPm')\n",
    "gottext2=re.sub(pattern8,\"I'm\",gottext2)\n",
    "\n",
    "pattern9 = re.compile('CAME OF THRONES 119')\n",
    "gottext2=re.sub(pattern9,\"\",gottext2)\n",
    "\n",
    "pattern10 = re.compile('0')\n",
    "gottext2=re.sub(pattern10,\"o\",gottext2)\n",
    "\n",
    "pattern11 = re.compile('A GAM[EL],* OF THRON[LE]S [0-9][0-9]*[0-9]*')\n",
    "gottext2=re.sub(pattern11,\"of\",gottext2)\n",
    "\n",
    "pattern12 = re.compile('[0-9][0-9]*[0-9]* GLORG[LE] R.R. MARTIN')\n",
    "gottext2=re.sub(pattern12,\"of\",gottext2)\n",
    "\n",
    "pattern13 = re.compile('402 GEORGE R.R. MARUN')\n",
    "gottext2=re.sub(pattern13,\"\",gottext2)\n",
    "\n",
    "pattern14 = re.compile('11')\n",
    "gottext2=re.sub(pattern14,\"\",gottext2)\n",
    "\n",
    "pattern15 = re.compile('[0-9][0-9]*[0-9]* GEORGL R.R. MARTIN')\n",
    "gottext2=re.sub(pattern15,\"\",gottext2)\n",
    "\n",
    "pattern16 = re.compile('1')\n",
    "gottext2=re.sub(pattern16,\"I\",gottext2)\n",
    "\n",
    "pattern17 = re.compile('[0-9][0-9]*[0-9]* GEORGE R.R. MARTM')\n",
    "gottext2=re.sub(pattern17,\"\",gottext2)\n",
    "\n",
    "pattern18 = re.compile('1oo GLORGE R.R. MARTIN')\n",
    "gottext2=re.sub(pattern18,\"\",gottext2)\n",
    "\n",
    "pattern19 = re.compile('[0-9][0-9]*[0-9]* GEORGE RA. MARTIN')\n",
    "gottext2=re.sub(pattern19,\"\",gottext2)\n",
    "\n",
    "pattern20 = re.compile('[0-9][0-9]*[0-9]* GEORGE R.R. MARTIN')\n",
    "gottext2=re.sub(pattern20,\"\",gottext2)\n",
    "\n",
    "pattern21 = re.compile('[0-9][0-9]*[0-9]* GLORGL R.R. MARTIN')\n",
    "gottext2=re.sub(pattern21,\"\",gottext2)\n",
    "\n",
    "pattern22 = re.compile('A [CG]A[NM]4*[EL] OF THRON[EF]S [0-9][0-9]*[0-9]')\n",
    "gottext2=re.sub(pattern22,\"\",gottext2)\n",
    "\n",
    "pattern23 = re.compile('9\\' \". . .')\n",
    "gottext2=re.sub(pattern23,\"\",gottext2)\n",
    "\n",
    "pattern24 = re.compile('[0-9][0-9]*[0-9]* GEORGE R.R. MARUN')\n",
    "gottext2=re.sub(pattern24,\"\",gottext2)\n",
    "\n",
    "pattern25 = re.compile('A GAME OF TFIRONES 535')\n",
    "gottext2=re.sub(pattern25,\"\",gottext2)\n",
    "\n",
    "pattern26 = re.compile('[0-9][0-9]*[0-9]* GLORGL R.R. MARTIN')\n",
    "gottext2=re.sub(pattern26,\"\",gottext2)\n",
    "\n",
    "pattern27 = re.compile('26o GLORGL R.R. MARTIN')\n",
    "gottext2=re.sub(pattern27,\"\",gottext2)\n",
    "\n",
    "pattern28 = re.compile('ofo[0-9]')\n",
    "gottext2=re.sub(pattern28,\"\",gottext2)\n",
    "\n",
    "pattern29 = re.compile('m7ord')\n",
    "gottext2=re.sub(pattern29,\"mlord\",gottext2)\n",
    "\n",
    "pattern30 = re.compile('4o ')\n",
    "gottext2=re.sub(pattern30,\"\",gottext2)\n",
    "\n",
    "pattern31 = re.compile('Red4yne')\n",
    "gottext2=re.sub(pattern31,\"Redyne\",gottext2)\n",
    "\n",
    "pattern32 = re.compile('65o GLORGL R.R. MARTIN')\n",
    "gottext2=re.sub(pattern32,\"\",gottext2)\n",
    "\n",
    "pattern33 = re.compile('Red4yne')\n",
    "gottext2=re.sub(pattern33,\"Redyne\",gottext2)\n",
    "\n",
    "pattern34 = re.compile(\"A GAME OF 'FHRONES 679\")\n",
    "gottext2=re.sub(pattern34,\"\",gottext2)\n",
    "\n",
    "pattern35 = re.compile(\" I 9\")\n",
    "gottext2=re.sub(pattern35,\"\",gottext2)\n",
    "\n",
    "pattern36=re.compile(\"\\'\\'\")\n",
    "gottext2=re.sub(pattern36,\"\",gottext2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad134854",
   "metadata": {},
   "source": [
    "    Using nltk word tokenizer the text was broken in word tokens.  There remains one pattern that was not able to be deleted by using Regular expressions and was removed directly after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "313c627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "gottokens = nltk.word_tokenize(gottext2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e103702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a loop to manually remove an unwanted token.\n",
    "fixedgottokesn =[]\n",
    "for token in gottokens:\n",
    "    if token == \"''\":\n",
    "        continue\n",
    "    else:\n",
    "        fixedgottokesn.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a799a",
   "metadata": {},
   "source": [
    "    To visualize and understand how each step is reducing the Frequency distribution and total vocabulary a function is created for easier access to this information.  It simply takes the tokenized text as input and prints out the FreqDist function provided by nltk then using a loop displays the most common 15 tokens and the number of times they appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "403afe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "#creating a function to display\n",
    "#frequeny distribution and total vocab count\n",
    "#also prints most common 15 tolkens\n",
    "def vocab(list):\n",
    "    ndist=FreqDist(list)\n",
    "    nitems = ndist.most_common(15)\n",
    "    print(FreqDist(list))\n",
    "    for item in nitems:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c6f688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp:\n",
      "<FreqDist with 7129 samples and 101213 outcomes>\n",
      "(',', 5658)\n",
      "('.', 4642)\n",
      "('the', 3312)\n",
      "('’', 3111)\n",
      "('“', 2437)\n",
      "('”', 2413)\n",
      "('to', 1844)\n",
      "('and', 1808)\n",
      "('a', 1579)\n",
      "('Harry', 1318)\n",
      "('of', 1239)\n",
      "('he', 1208)\n",
      "('was', 1179)\n",
      "('s', 1002)\n",
      "('in', 931)\n",
      "None\n",
      "got:\n",
      "<FreqDist with 13187 samples and 353775 outcomes>\n",
      "('.', 25060)\n",
      "(',', 21040)\n",
      "('the', 15126)\n",
      "('and', 8419)\n",
      "('``', 8167)\n",
      "('to', 6490)\n",
      "('a', 5853)\n",
      "('of', 5825)\n",
      "('his', 4597)\n",
      "('was', 3890)\n",
      "('he', 3428)\n",
      "('her', 3416)\n",
      "('I', 3219)\n",
      "('in', 3092)\n",
      "('had', 2855)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Looking at the original tokens created\n",
    "print('hp:')\n",
    "print(vocab(hptokens))\n",
    "print('got:')\n",
    "print(vocab(fixedgottokesn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32805758",
   "metadata": {},
   "source": [
    "\t(2a)To reduce the vocabulary and combine words that start a sentence and are used in a sentence (non-proper nouns) all the text will be converted to lower case.  This was accomplished by creating a function that takes each token from the list and uses .lower() on the token saves it as a temporary variable then adds it to a temporary list and returns the temporary list.  This function was used on each of the tokenized books and saved as a new list.  In some cases, the upper-case letters could give valuable information, they specify proper nouns and the start of sentences.  The word used to begin each sentence is not important to this analysis and the proper nouns are already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2330b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to take a list \n",
    "#make each token lowercase\n",
    "#place the lowercase tokens into a list\n",
    "def lc(list):\n",
    "    lclist =[]\n",
    "    for token in list:\n",
    "        lcw = token.lower()\n",
    "        lclist.append(lcw) \n",
    "    return lclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "632d68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the lowercase function\n",
    "# creating new tokenized list\n",
    "#only lowercase characters remain\n",
    "hptokens_lc=lc(hptokens)\n",
    "gottokens_lc=lc(fixedgottokesn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "187e539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp:\n",
      "<FreqDist with 6295 samples and 101213 outcomes>\n",
      "(',', 5658)\n",
      "('.', 4642)\n",
      "('the', 3626)\n",
      "('’', 3111)\n",
      "('“', 2437)\n",
      "('”', 2413)\n",
      "('and', 1920)\n",
      "('to', 1854)\n",
      "('he', 1756)\n",
      "('a', 1684)\n",
      "('harry', 1319)\n",
      "('of', 1256)\n",
      "('was', 1186)\n",
      "('it', 1168)\n",
      "('s', 1003)\n",
      "None\n",
      "got:\n",
      "<FreqDist with 11864 samples and 353775 outcomes>\n",
      "('.', 25060)\n",
      "(',', 21040)\n",
      "('the', 17689)\n",
      "('and', 8834)\n",
      "('``', 8167)\n",
      "('to', 6546)\n",
      "('a', 6384)\n",
      "('of', 5838)\n",
      "('he', 5402)\n",
      "('his', 5073)\n",
      "('was', 3915)\n",
      "('her', 3642)\n",
      "('you', 3472)\n",
      "('in', 3229)\n",
      "('i', 3220)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('hp:')\n",
    "print(vocab(hptokens_lc))\n",
    "print('got:')\n",
    "print(vocab(gottokens_lc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4ed9d",
   "metadata": {},
   "source": [
    "\t(2a)As is shown above the top 15 tokens of each book are comprised of mostly stop words and punctuation.  The objective is to compare vocabulary used between two different fantasy books one for adults and one for younger audiences, the punctuation must be removed.  Using the punctuation list imported from string and additional punctuation marks found not to be included were used in a function.  The function takes each token and checks if the token is not in the punctuation list created.  If the token is not on the punctation list, that token is placed in a new list.  The new list is returned. This function is run on the lowercase only tokenized list and saved as a new list.  The added punctuation was important because after initially running the “normal” punctuation list the added ones were still found to be in the most common 15.  This is taking up a place of some frequent vocabulary of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1558f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to remove punctuation from a list\n",
    "from string import punctuation\n",
    "punctuation = punctuation + '’”“`''`—'\n",
    "def remove_punc(tokens):\n",
    "    return [t for t in tokens if t not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d71f8f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hptokens_lcp=remove_punc(hptokens_lc)\n",
    "gottokens_lcp=remove_punc(gottokens_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2d51bb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp:\n",
      "<FreqDist with 6281 samples and 80580 outcomes>\n",
      "('the', 3626)\n",
      "('and', 1920)\n",
      "('to', 1854)\n",
      "('he', 1756)\n",
      "('a', 1684)\n",
      "('harry', 1319)\n",
      "('of', 1256)\n",
      "('was', 1186)\n",
      "('it', 1168)\n",
      "('s', 1003)\n",
      "('you', 992)\n",
      "('in', 963)\n",
      "('his', 936)\n",
      "('i', 921)\n",
      "('t', 833)\n",
      "None\n",
      "got:\n",
      "<FreqDist with 11851 samples and 296750 outcomes>\n",
      "('the', 17689)\n",
      "('and', 8834)\n",
      "('to', 6546)\n",
      "('a', 6384)\n",
      "('of', 5838)\n",
      "('he', 5402)\n",
      "('his', 5073)\n",
      "('was', 3915)\n",
      "('her', 3642)\n",
      "('you', 3472)\n",
      "('in', 3229)\n",
      "('i', 3220)\n",
      "('it', 3058)\n",
      "('had', 2868)\n",
      "('she', 2865)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('hp:')\n",
    "print(vocab(hptokens_lcp))\n",
    "print('got:')\n",
    "print(vocab(gottokens_lcp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e883d7b",
   "metadata": {},
   "source": [
    "    (2a)(2d)As seen above the topmost common tokens are mostly comprised of stop words.  The most common tokens from both texts are very similar and will not yield good results since the vocabulary is to be compared.  To find the vocabulary that is used most often, but that gives actual meaning and understanding into the audiences age that the books are intended for, the stop words must be removed.  A list of stop words was acquired from nltk.  Since the tokenization process that was used created ‘s and n’t in separate tokens, they were added to the stop list.  The list of lowercase tokens that had punctuation removed were run through a loop each token was compared to the stop list.  If the token did not appear in the stop list, it was added into a new list.  This list no longer contained any of the tokens that appear on the stop list created.  Once this was preformed initially both lists had said as either the most common or second most common token.  Since this is a very common word in fiction books and added little information gain it was also added to the stop list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13efe185",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords = nltkstopwords +  [\"'s\",\"n't\",\"said\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "de267ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hp_stop=[]\n",
    "for token in hptokens_lcp:\n",
    "    if token not in stopwords:\n",
    "        hp_stop.append(token)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "got_stop=[]\n",
    "for token in gottokens_lcp:\n",
    "    if token not in stopwords:\n",
    "        got_stop.append(token)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3d783944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp:\n",
      "<FreqDist with 6131 samples and 40231 outcomes>\n",
      "('harry', 1319)\n",
      "('ron', 425)\n",
      "('hagrid', 370)\n",
      "('hermione', 267)\n",
      "('back', 255)\n",
      "('one', 251)\n",
      "('...', 238)\n",
      "('got', 206)\n",
      "('could', 198)\n",
      "('get', 194)\n",
      "('like', 192)\n",
      "('know', 181)\n",
      "('see', 178)\n",
      "('professor', 177)\n",
      "('looked', 169)\n",
      "got:\n",
      "<FreqDist with 11717 samples and 148838 outcomes>\n",
      "('lord', 1301)\n",
      "('would', 1156)\n",
      "('ser', 973)\n",
      "('could', 840)\n",
      "('jon', 830)\n",
      "('one', 792)\n",
      "('ned', 785)\n",
      "('man', 775)\n",
      "('king', 736)\n",
      "('back', 691)\n",
      "('father', 635)\n",
      "('tyrion', 615)\n",
      "('like', 597)\n",
      "('hand', 584)\n",
      "('men', 575)\n"
     ]
    }
   ],
   "source": [
    "print('hp:')\n",
    "vocab(hp_stop)\n",
    "print('got:')\n",
    "vocab(got_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c34c25",
   "metadata": {},
   "source": [
    "\t(2a)To reduce the vocabulary and combine words such as different and differently the porter stemmer was used to break the tokens down to its root.  This gives the chance to move a more commonly used word that has different suffix/prefix up in frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d18feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Porter stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e99d55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_stem =[]\n",
    "for token in hp_stop:\n",
    "    x=ps.stem(token)\n",
    "    hp_stem.append(x)\n",
    "    \n",
    "got_stem =[]\n",
    "for token in got_stop:\n",
    "    x=ps.stem(token)\n",
    "    got_stem.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "28df31c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp:\n",
      "<FreqDist with 4557 samples and 40231 outcomes>\n",
      "('harri', 1319)\n",
      "('ron', 425)\n",
      "('look', 404)\n",
      "('hagrid', 370)\n",
      "('hermion', 267)\n",
      "('go', 263)\n",
      "('one', 262)\n",
      "('back', 261)\n",
      "('...', 238)\n",
      "('get', 235)\n",
      "('know', 212)\n",
      "('like', 211)\n",
      "('got', 206)\n",
      "('could', 198)\n",
      "('see', 183)\n",
      "None\n",
      "got:\n",
      "<FreqDist with 7725 samples and 148838 outcomes>\n",
      "('lord', 1438)\n",
      "('would', 1156)\n",
      "('ser', 975)\n",
      "('could', 840)\n",
      "('hand', 833)\n",
      "('jon', 830)\n",
      "('one', 823)\n",
      "('look', 814)\n",
      "('ned', 785)\n",
      "('man', 781)\n",
      "('king', 775)\n",
      "('back', 719)\n",
      "('like', 715)\n",
      "('father', 651)\n",
      "('eye', 630)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('hp:')\n",
    "print(vocab(hp_stem))\n",
    "print('got:')\n",
    "print(vocab(got_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eedb644",
   "metadata": {},
   "source": [
    "\tBelow are the top 50 most common tokens from each text.  This will be used with the bigrams to examine the vocabulary and compare them to see the difference between fantasy books written for particular age ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "96f3670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('harri', 1319)\n",
      "('ron', 425)\n",
      "('look', 404)\n",
      "('hagrid', 370)\n",
      "('hermion', 267)\n",
      "('go', 263)\n",
      "('one', 262)\n",
      "('back', 261)\n",
      "('...', 238)\n",
      "('get', 235)\n",
      "('know', 212)\n",
      "('like', 211)\n",
      "('got', 206)\n",
      "('could', 198)\n",
      "('see', 183)\n",
      "('professor', 177)\n",
      "('tri', 170)\n",
      "('snape', 168)\n",
      "('dumbledor', 158)\n",
      "('think', 146)\n",
      "('around', 141)\n",
      "('dudley', 137)\n",
      "('want', 135)\n",
      "('time', 134)\n",
      "('say', 129)\n",
      "('come', 129)\n",
      "('someth', 128)\n",
      "('never', 126)\n",
      "('malfoy', 126)\n",
      "('uncl', 122)\n",
      "('head', 121)\n",
      "('door', 121)\n",
      "('even', 120)\n",
      "('eye', 120)\n",
      "('right', 120)\n",
      "('yeh', 119)\n",
      "('nevil', 116)\n",
      "('turn', 115)\n",
      "('vernon', 115)\n",
      "('well', 114)\n",
      "('hand', 114)\n",
      "('quirrel', 113)\n",
      "('face', 111)\n",
      "('would', 108)\n",
      "('first', 108)\n",
      "('potter', 107)\n",
      "('dursley', 106)\n",
      "('thing', 103)\n",
      "('way', 101)\n",
      "('ask', 101)\n"
     ]
    }
   ],
   "source": [
    "fdisthp = FreqDist(hp_stem)\n",
    "hptopkeys = fdisthp.most_common(50)\n",
    "for pair in hptopkeys:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "15481c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lord', 1301)\n",
      "('would', 1156)\n",
      "('ser', 973)\n",
      "('could', 840)\n",
      "('jon', 830)\n",
      "('one', 792)\n",
      "('ned', 785)\n",
      "('man', 775)\n",
      "('king', 736)\n",
      "('back', 691)\n",
      "('father', 635)\n",
      "('tyrion', 615)\n",
      "('like', 597)\n",
      "('hand', 584)\n",
      "('men', 575)\n",
      "('eyes', 561)\n",
      "('bran', 546)\n",
      "('told', 502)\n",
      "('see', 498)\n",
      "('catelyn', 491)\n",
      "('arya', 459)\n",
      "('even', 447)\n",
      "('face', 443)\n",
      "('boy', 439)\n",
      "('brother', 437)\n",
      "('know', 423)\n",
      "('sansa', 422)\n",
      "('dany', 413)\n",
      "('robb', 409)\n",
      "('old', 405)\n",
      "('looked', 405)\n",
      "('never', 404)\n",
      "('robert', 403)\n",
      "('well', 397)\n",
      "('time', 393)\n",
      "('stark', 393)\n",
      "('black', 390)\n",
      "('long', 389)\n",
      "('yet', 378)\n",
      "('head', 374)\n",
      "('away', 369)\n",
      "('made', 361)\n",
      "('night', 354)\n",
      "('maester', 344)\n",
      "('lannister', 344)\n",
      "('lady', 343)\n",
      "('come', 337)\n",
      "('took', 337)\n",
      "('thought', 336)\n",
      "('sword', 335)\n"
     ]
    }
   ],
   "source": [
    "fdistgot = FreqDist(got_stop)\n",
    "gottopkeys = fdistgot.most_common(50)\n",
    "for pair in gottopkeys:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb757dda",
   "metadata": {},
   "source": [
    "\tNltk’s bigrams was used on the list of tokens containing on lower case characters and placed in a list. The bigram assocmeasures() was saved into a variable and that variable was saved as finder and used to create a score for the bigrams by frequency.  As expected much normalization needs to occur before any information is gained from the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "13b7e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpbigrams = list(nltk.bigrams(hptokens_lc))\n",
    "gotbigrams = list(nltk.bigrams(gottokens_lc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a156b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for bigrams and bigram measures\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fbb13b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder = BigramCollocationFinder.from_words(hptokens_lc)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "da8bbba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('.', '“'), 0.015343878750753362)\n",
      "(('’', 's'), 0.009870273581456928)\n",
      "(('’', 't'), 0.008230168061415036)\n",
      "((',', '”'), 0.0070050289982512125)\n",
      "(('”', '“'), 0.00596761285605604)\n",
      "(('”', 'said'), 0.0055526463991779716)\n",
      "(('?', '”'), 0.005167320403505479)\n",
      "((',', 'and'), 0.004021222570223193)\n",
      "(('.', 'he'), 0.0038532599567249266)\n",
      "((',', 'but'), 0.0029047651981464833)\n",
      "(('!', '”'), 0.002894885044411291)\n",
      "(('of', 'the'), 0.002825723968264946)\n",
      "(('i', '’'), 0.0026972819697074486)\n",
      "(('’', 'd'), 0.0026676415085018722)\n",
      "(('in', 'the'), 0.0026577613547666802)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder = BigramCollocationFinder.from_words(hptokens_lc)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "# scores are sorted in decreasing frequency\n",
    "for bscore in scored[:15]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ddb0209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('.', '``'), 0.019633948130874143)\n",
      "((',', 'and'), 0.007725249099003604)\n",
      "(('.', 'the'), 0.00555437778248887)\n",
      "(('.', 'he'), 0.004692247897675076)\n",
      "(('.', '.'), 0.004655501377994488)\n",
      "(('of', 'the'), 0.004293689491908699)\n",
      "((',', 'but'), 0.0032393470426118295)\n",
      "((',', 'the'), 0.003160200692530563)\n",
      "(('in', 'the'), 0.00312062751748993)\n",
      "(('``', 'i'), 0.002934068263726945)\n",
      "((',', 'he'), 0.002654229383082468)\n",
      "(('.', 'she'), 0.0023291640166772667)\n",
      "(('said', '.'), 0.00218500459331496)\n",
      "(('and', 'the'), 0.0020295385485124724)\n",
      "(('to', 'the'), 0.002006925305632111)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder2 = BigramCollocationFinder.from_words(gottokens_lc)\n",
    "scored2 = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "# scores are sorted in decreasing frequency\n",
    "for bscore in scored2[:15]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced362b",
   "metadata": {},
   "source": [
    "\t(2a)Since the above bigrams are not giving much information in terms of vocabulary punctuation must first be removed.  In this case to remove any token that contains non-alphabetical characters the alpha_filter is used.  This will allow only vocabulary to remain which is what is required for the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "177678dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "51eb712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('of', 'the'), 0.002825723968264946)\n",
      "(('in', 'the'), 0.0026577613547666802)\n",
      "(('on', 'the'), 0.0021143528993311134)\n",
      "(('it', 'was'), 0.0020353116694495767)\n",
      "(('he', 'was'), 0.0019266299783624633)\n",
      "(('to', 'the'), 0.0016993864424530445)\n",
      "(('out', 'of'), 0.0014425024453380494)\n",
      "(('at', 'the'), 0.0013931016766620889)\n",
      "(('said', 'harry'), 0.0013338207542509361)\n",
      "(('he', 'had'), 0.0011362176795470939)\n",
      "(('said', 'ron'), 0.001076936757135941)\n",
      "(('to', 'be'), 0.001067056603400749)\n",
      "(('uncle', 'vernon'), 0.001067056603400749)\n",
      "(('he', 'said'), 0.0010472962959303647)\n",
      "(('in', 'a'), 0.0010374161421951725)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:15]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cc2039a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('of', 'the'), 0.004293689491908699)\n",
      "(('in', 'the'), 0.00312062751748993)\n",
      "(('and', 'the'), 0.0020295385485124724)\n",
      "(('to', 'the'), 0.002006925305632111)\n",
      "(('it', 'was'), 0.0016507667302664124)\n",
      "(('on', 'the'), 0.0015631404141050102)\n",
      "(('he', 'was'), 0.0015094339622641509)\n",
      "(('from', 'the'), 0.0013313546745813017)\n",
      "(('the', 'king'), 0.0012295950816196735)\n",
      "(('he', 'had'), 0.0011815419404989046)\n",
      "(('at', 'the'), 0.0011504487315384072)\n",
      "(('had', 'been'), 0.0011023955904176383)\n",
      "(('of', 'his'), 0.0011023955904176383)\n",
      "(('with', 'a'), 0.001062822415377005)\n",
      "(('he', 'said'), 0.0009751960992156032)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder2.apply_word_filter(alpha_filter)\n",
    "scored2 = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored2[:15]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ed5d5",
   "metadata": {},
   "source": [
    "\t(2a)(2d)As seen above the bigrams with the highest scores from both texts are comprised of mostly stop words.  Using a lambda function the stopwords list created before was used to remove stop words.  This portion also removes any bigrams were the frequency is less than 5 in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5f7f1b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('uncle', 'vernon'), 0.001067056603400749)\n",
      "(('professor', 'mcgonagall'), 0.0008990939899024829)\n",
      "(('aunt', 'petunia'), 0.0005137679942299903)\n",
      "(('mr.', 'dursley'), 0.0002964046120557636)\n",
      "(('harry', 'potter'), 0.0002667641508501872)\n",
      "(('harry', 'could'), 0.00021736338217422663)\n",
      "(('harry', 'looked'), 0.00021736338217422663)\n",
      "(('looked', 'like'), 0.0002074832284390345)\n",
      "(('professor', 'dumbledore'), 0.0002074832284390345)\n",
      "(('common', 'room'), 0.0001976030747038424)\n",
      "(('could', 'see'), 0.0001976030747038424)\n",
      "(('harry', 'felt'), 0.0001976030747038424)\n",
      "(('first', 'years'), 0.00018772292096865028)\n",
      "(('mrs.', 'dursley'), 0.00018772292096865028)\n",
      "(('professor', 'quirrell'), 0.00018772292096865028)\n",
      "(('hermione', 'granger'), 0.00016796261349826604)\n",
      "(('harry', 'asked'), 0.00015808245976307392)\n",
      "(('harry', 'thought'), 0.00015808245976307392)\n",
      "(('mr.', 'ollivander'), 0.00015808245976307392)\n",
      "(('privet', 'drive'), 0.00015808245976307392)\n",
      "(('great', 'hall'), 0.0001482023060278818)\n",
      "(('nimbus', 'two'), 0.0001482023060278818)\n",
      "(('professor', 'flitwick'), 0.0001482023060278818)\n",
      "(('first', 'time'), 0.00013832215229268968)\n",
      "(('invisibility', 'cloak'), 0.00013832215229268968)\n",
      "(('madam', 'pomfrey'), 0.00013832215229268968)\n",
      "(('get', 'past'), 0.00012844199855749757)\n",
      "(('two', 'thousand'), 0.00012844199855749757)\n",
      "(('harry', 'told'), 0.00011856184482230543)\n",
      "(('madam', 'hooch'), 0.00011856184482230543)\n",
      "(('mr.', 'potter'), 0.00011856184482230543)\n",
      "(('mrs.', 'norris'), 0.00011856184482230543)\n",
      "(('next', 'morning'), 0.00011856184482230543)\n",
      "(('tell', 'yeh'), 0.00011856184482230543)\n",
      "(('never', 'seen'), 0.00010868169108711332)\n",
      "(('nicolas', 'flamel'), 0.00010868169108711332)\n",
      "(('could', 'hear'), 9.88015373519212e-05)\n",
      "(('harry', 'saw'), 9.88015373519212e-05)\n",
      "(('harry', 'tried'), 9.88015373519212e-05)\n",
      "(('house', 'cup'), 9.88015373519212e-05)\n",
      "(('leaky', 'cauldron'), 9.88015373519212e-05)\n",
      "(('albus', 'dumbledore'), 8.892138361672908e-05)\n",
      "(('every', 'flavor'), 8.892138361672908e-05)\n",
      "(('five', 'minutes'), 8.892138361672908e-05)\n",
      "(('go', 'back'), 8.892138361672908e-05)\n",
      "(('hagrid', 'looked'), 8.892138361672908e-05)\n",
      "(('harry', 'heard'), 8.892138361672908e-05)\n",
      "(('harry', 'knew'), 8.892138361672908e-05)\n",
      "(('harry', 'noticed'), 8.892138361672908e-05)\n",
      "(('harry', 'turned'), 8.892138361672908e-05)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter to remove stop words\n",
    "finder.apply_freq_filter(5)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:50]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f141eacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('ser', 'jorah'), 0.0004353049254469649)\n",
      "(('lord', 'tywin'), 0.00035333192000565333)\n",
      "(('maester', 'luwin'), 0.0003335453324853367)\n",
      "(('ser', 'rodrik'), 0.0003194120556851106)\n",
      "(('khal', 'drogo'), 0.0003081054342449297)\n",
      "(('lord', 'eddard'), 0.0003024521235248392)\n",
      "(('could', 'see'), 0.00026005229312416083)\n",
      "(('lord', 'father'), 0.00023743905024379902)\n",
      "(('tyrion', 'lannister'), 0.0002261324288036181)\n",
      "(('septa', 'mordane'), 0.00021199915200339198)\n",
      "(('eddard', 'stark'), 0.00018090594304289448)\n",
      "(('ser', 'barristan'), 0.00018090594304289448)\n",
      "(('grand', 'maester'), 0.00017807928768284926)\n",
      "(('seven', 'kingdoms'), 0.00017807928768284926)\n",
      "(('lord', 'commander'), 0.00017525263232280404)\n",
      "(('lord', 'renly'), 0.00017242597696275882)\n",
      "(('could', 'feel'), 0.00016394601088262314)\n",
      "(('ser', 'alliser'), 0.00016394601088262314)\n",
      "(('old', 'man'), 0.00015546604480248745)\n",
      "(('maester', 'pycelle'), 0.00015263938944244223)\n",
      "(('would', 'never'), 0.000149812734082397)\n",
      "(('maz', 'duur'), 0.0001469860787223518)\n",
      "(('jon', 'arryn'), 0.00014415942336230654)\n",
      "(('mirri', 'maz'), 0.00014415942336230654)\n",
      "(('casterly', 'rock'), 0.00014133276800226132)\n",
      "(('jon', 'snow'), 0.00014133276800226132)\n",
      "(('ser', 'vardis'), 0.00013567945728217088)\n",
      "(('old', 'nan'), 0.00013285280192212564)\n",
      "(('catelyn', 'stark'), 0.00013002614656208042)\n",
      "(('maester', 'aemon'), 0.00012437283584198998)\n",
      "(('old', 'bear'), 0.00011871952512189951)\n",
      "(('first', 'time'), 0.00011589286976185429)\n",
      "(('theon', 'greyjoy'), 0.00011589286976185429)\n",
      "(('lady', 'stark'), 0.00011306621440180906)\n",
      "(('ser', 'gregor'), 0.00011306621440180906)\n",
      "(('jaime', 'lannister'), 0.00011023955904176384)\n",
      "(('king', 'robert'), 0.0001074129036817186)\n",
      "(('never', 'seen'), 0.00010458624832167338)\n",
      "(('prince', 'joffrey'), 0.00010458624832167338)\n",
      "(('could', 'hear'), 0.00010175959296162815)\n",
      "(('grey', 'wind'), 0.00010175959296162815)\n",
      "(('lord', 'stannis'), 0.00010175959296162815)\n",
      "(('red', 'keep'), 9.327962688149248e-05)\n",
      "(('tywin', 'lannister'), 9.045297152144724e-05)\n",
      "(('sandor', 'clegane'), 8.762631616140202e-05)\n",
      "(('ser', 'loras'), 8.762631616140202e-05)\n",
      "(('vaes', 'dothrak'), 8.762631616140202e-05)\n",
      "(('benjen', 'stark'), 8.479966080135679e-05)\n",
      "(('lord', 'arryn'), 8.479966080135679e-05)\n",
      "(('samwell', 'tarly'), 8.479966080135679e-05)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter to remove stop words\n",
    "finder2.apply_freq_filter(5)\n",
    "finder2.apply_word_filter(lambda w: w in stopwords)\n",
    "scored2 = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored2[:50]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719a9fe",
   "metadata": {},
   "source": [
    "    (2b) When looking at the lists of most frequent tokens and the most frequent bigrams they are a strong representation of the text themselves.  It would be possible to remove Proper Nouns (character names) to increase the vocabulary gained.  Although this process would be quite extensive as there are many included, and as they were removed more would most likely take their place knowing these books have a very high number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70744b7c",
   "metadata": {},
   "source": [
    "    The bigrams are run with pointwise mutual information and the top 50 are displayed.  This provides information that these words express a unique concept when used together and may give a better expression of intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "beeb5c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('boa', 'constrictor'), 14.305106984117373)\n",
      "(('goal', 'posts'), 13.819680156947133)\n",
      "(('diagon', 'alley'), 13.627035079004735)\n",
      "(('flavor', 'beans'), 13.457110077562422)\n",
      "(('hospital', 'wing'), 13.364000673170938)\n",
      "(('smelting', 'stick'), 13.305106984117373)\n",
      "(('shooting', 'stars'), 13.194075671728626)\n",
      "(('bloody', 'baron'), 12.872147576841268)\n",
      "(('lee', 'jordan'), 12.845675365480078)\n",
      "(('marcus', 'flint'), 12.819680156947129)\n",
      "(('leaky', 'cauldron'), 12.457110077562422)\n",
      "(('privet', 'drive'), 12.305106984117376)\n",
      "(('marble', 'staircase'), 12.248523455751005)\n",
      "(('chocolate', 'frogs'), 12.211997579725892)\n",
      "(('seamus', 'finnigan'), 12.082714562780925)\n",
      "(('fat', 'lady'), 11.912789561338617)\n",
      "(('portrait', 'hole'), 11.819680156947133)\n",
      "(('vault', 'seven'), 11.64975515550482)\n",
      "(('three-headed', 'dog'), 11.546661662540716)\n",
      "(('nicolas', 'flamel'), 11.479136383892424)\n",
      "(('madam', 'hooch'), 11.457110077562422)\n",
      "(('madam', 'malkin'), 11.457110077562422)\n",
      "(('madam', 'pomfrey'), 11.457110077562422)\n",
      "(('nearly', 'headless'), 11.417581713375785)\n",
      "(('car', 'crash'), 11.305106984117373)\n",
      "(('platform', 'nine'), 11.291644724310814)\n",
      "(('miss', 'granger'), 11.090094093146524)\n",
      "(('mrs.', 'norris'), 11.052126242947503)\n",
      "(('mrs.', 'figg'), 10.974958382425044)\n",
      "(('every', 'flavor'), 10.899114624441538)\n",
      "(('white', 'pieces'), 10.82484186206291)\n",
      "(('invisibility', 'cloak'), 10.769054083877164)\n",
      "(('entrance', 'hall'), 10.690397140002167)\n",
      "(('forbidden', 'forest'), 10.649755155504817)\n",
      "(('sorting', 'hat'), 10.627035079004735)\n",
      "(('fifty', 'points'), 10.569043356245562)\n",
      "(('fast', 'asleep'), 10.568141389951167)\n",
      "(('number', 'four'), 10.539572237754399)\n",
      "(('aunt', 'petunia'), 10.472216969952633)\n",
      "(('dark', 'arts'), 10.329354530364052)\n",
      "(('leaned', 'forward'), 10.319606553812486)\n",
      "(('seven', 'hundred'), 10.184091583156007)\n",
      "(('mr.', 'ollivander'), 10.135181982675062)\n",
      "(('nimbus', 'two'), 10.012325234889527)\n",
      "(('common', 'room'), 9.965969599197786)\n",
      "(('unicorn', 'blood'), 9.965969599197786)\n",
      "(('third', 'floor'), 9.912789561338613)\n",
      "(('trophy', 'room'), 9.904569054533642)\n",
      "(('five', 'minutes'), 9.89006948483853)\n",
      "(('ten', 'minutes'), 9.8132538877877)\n"
     ]
    }
   ],
   "source": [
    "### pointwise mutual information\n",
    "finder.apply_freq_filter(5)\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8844b86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('yi', 'ti'), 16.110544481138117)\n",
      "(('helman', 'tallhart'), 15.84751007530432)\n",
      "(('mance', 'rayder'), 15.110544481138117)\n",
      "(('masha', 'heddle'), 15.110544481138117)\n",
      "(('aron', 'santagar'), 14.99951316874937)\n",
      "(('tobho', 'mott'), 14.947045748855235)\n",
      "(('lyn', 'corbray'), 14.940619479695805)\n",
      "(('arthur', 'dayne'), 14.89997749519846)\n",
      "(('dosh', 'khaleen'), 14.625117653967877)\n",
      "(('balon', 'swann'), 14.625117653967875)\n",
      "(('donal', 'noye'), 14.625117653967873)\n",
      "(('willis', 'wode'), 14.525581980416959)\n",
      "(('karyl', 'vance'), 14.414550668028216)\n",
      "(('huny', 'huny'), 14.373578886971908)\n",
      "(('hallis', 'mollen'), 14.345009734775138)\n",
      "(('galbart', 'glover'), 14.236075363221975)\n",
      "(('jason', 'mallister'), 14.21745968505463)\n",
      "(('two-handed', 'greatsword'), 14.142965958830494)\n",
      "(('roose', 'bolton'), 14.10208290238992)\n",
      "(('bowen', 'marsh'), 14.040155153246719)\n",
      "(('flea', 'bottom'), 13.97304095738818)\n",
      "(('moat', 'cailin'), 13.908910619968466)\n",
      "(('janos', 'slynt'), 13.870230151804405)\n",
      "(('marq', 'piper'), 13.815088597611943)\n",
      "(('lemon', 'cakes'), 13.732032857884386)\n",
      "(('beric', 'dondarrion'), 13.716265542026072)\n",
      "(('raymun', 'darry'), 13.625117653967875)\n",
      "(('meryn', 'trant'), 13.574491580897908)\n",
      "(('vaes', 'dothrak'), 13.432472576025477)\n",
      "(('jaremy', 'rykker'), 13.410104762997024)\n",
      "(('ilyn', 'payne'), 13.388078456667024)\n",
      "(('cuts', 'deeper'), 13.336548156026943)\n",
      "(('crowned', 'stag'), 13.262547574583166)\n",
      "(('fermented', 'mare'), 13.184545062581893)\n",
      "(('vayon', 'poole'), 13.184545062581893)\n",
      "(('vardis', 'egen'), 12.847510075304323)\n",
      "(('samwell', 'tarly'), 12.844507587142797)\n",
      "(('maz', 'duur'), 12.732032857884384)\n",
      "(('syrio', 'forel'), 12.661643529992988)\n",
      "(('western', 'market'), 12.651112862500817)\n",
      "(('mirri', 'maz'), 12.52181515049404)\n",
      "(('yohn', 'royce'), 12.455192652525561)\n",
      "(('painted', 'vest'), 12.388078456667024)\n",
      "(('sandor', 'clegane'), 12.383853870805297)\n",
      "(('loras', 'tyrell'), 12.246606030714146)\n",
      "(('wet', 'nurse'), 12.177444006206748)\n",
      "(('rickard', 'karstark'), 12.06990249664077)\n",
      "(('bride', 'gift'), 12.040155153246719)\n",
      "(('haunted', 'forest'), 12.023081639887776)\n",
      "(('magister', 'illyrio'), 11.894384013513184)\n"
     ]
    }
   ],
   "source": [
    "### pointwise mutual information\n",
    "finder2.apply_freq_filter(5)\n",
    "scored2 = finder2.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored2[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd143f7",
   "metadata": {},
   "source": [
    "\t(2c) The PMI removed some of the names of characters that appeared in the bigrams.  This is because they are very common characters, and their names are not first and last each time they are used.  Although it does give new character names, that are only used in full as their replacements.  The PMI does give a good feel for the books themselves.  Capturing completely made-up names of people and places in the Harry Potter books any fan would be familiar with such as Diagon Alley, Chocolate Frogs, and three-headed dog.  While in the Game of Thrones book it showcases darker terms and an air of medieval times such as two-handed greatsword, flea bottom, and cuts deeper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6e5c57ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 4557 samples and 40231 outcomes>\n",
      "<FreqDist with 7725 samples and 148838 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(FreqDist(hp_stem))\n",
    "print(FreqDist(got_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156e5d4",
   "metadata": {},
   "source": [
    "    \tAs a review the goal is to see difference in the top tokens that would distinguish one fantasy book as being for adults (Game of Thrones) (GoT) and one for children (Harry Potter) (HP).  When looking at the top tokens after normalization we can see names in both, around the same number (13 in HP and 12 in GoT) the other words are telling of the books.  HP simple non descriptive words, (go, could, something) and Professor while GoT has some simple non descriptive words (looked, like) but also sword, king, lord, lady and maester a term from the book (a holy man).     “ …:” was left in as a token because it used 238 times in Harry Potter, this signifies almost overuse possibly because it is designed for children.  We see much more distinctive words from GoT which was intended for adults.  This is most likely caused by a variety of synonyms and a larger overall unique vocabulary.  This can also be seen in the Frequncy distribution from HP being 4557 while in GoT being 7725.  \n",
    "\n",
    "    In the bigrams 11 of the 50 are Harry something. This is Harry felt, Harry asked, Harry looked, etc..  It is telling of the book being all about Harry and his perspective of the world.  While we see in GoT there are a lot more names. There are also more complex world building bigrams like titles (lord commander, grand measter).  HP has places, yet they describe smaller settings such as, common room, great hall, private drive, and one playful “leaky cauldron”. while GoT has many places that are larger in scale, Seven Kingdoms, Casterly Rock, Grey Wind, and Red Keep. This shows the size of the world being created in the text is very different.  The smaller world being created is from the child’s fantasy book while the larger is of the adult fantasy book.\n",
    "\n",
    "    In the Pointwise mutual information, there are a lot of side character names that are always mentioned in full.  Then in HP there is portion of the story telling that is mystical, smelting stick, flavor beans, goal posts, chocolate frogs, forbidden forest, unicorn blood, dark arts.  We see one example that is comical for children but not so funny for adults “fat lady”.  This is a intricate world that has been created but in scales it is smaller.  When we look at the GoT PMI there are many more side characters.  So many that it is at times hard to see that they are proper nouns.  The next big separation between texts breaks through and that is exampled in yi ti, tobho mott, vaes Dothrak, and maz duur.  These seem like misspelling or possibly something went wrong in python coding.  But a small bit of understanding can shed light that this is Dothraki a language created for this series.  The PMI picks these words out because they don’t appear alone.  This sheds light into an aspect of fantasy that isn’t very often found in children’s books.  The writer creates another langue specifically for the book.\n",
    "\n",
    "\tOverall, the words presented in top words, bigrams, and the PMI bigrams, point to the size of the fictious world that was created in these texts.  While both are complex and other worldly.  The overall tokens apprehended point to the Game of Thrones world being much larger in scale and shown with the PMI in shear number of characters.  This largeness is more suited for an advanced reader.  The elements particular to the books pointed out by the PMI are fun and very understandable for Harry Potter, while in the Game of Thrones the PMI finds a language that was created for the books and a staggering number of side characters.\n",
    "    \n",
    "    The complexity difference in the findings showcases one of the largest differences in fantasy books written for adults vs children, the shear size of the worlds being created.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
